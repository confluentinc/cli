{
  "SQL": [],
  "Getting Started": [
    "SELECT 'Hello World';\n",
    "SHOW FUNCTIONS;\n",
    "SELECT CURRENT_TIMESTAMP;\n",
    "CREATE TABLE employee_information (\n    emp_id INT,\n    name VARCHAR,\n    dept_id INT\n) WITH ( \n    'connector' = 'filesystem',\n    'path' = '/path/to/something.csv',\n    'format' = 'csv'\n);\n",
    "SELECT * from employee_information WHERE dept_id = 1;\n",
    "SELECT \n   dept_id,\n   COUNT(*) as emp_count \nFROM employee_information \nGROUP BY dept_id;\n",
    "INSERT INTO department_counts\nSELECT \n   dept_id,\n   COUNT(*) as emp_count \nFROM employee_information;\n"
  ],
  "Overview": [
    "query:\n    values\n  | WITH withItem [ , withItem ]* query\n  | {\n        select\n      | selectWithoutFrom\n      | query UNION [ ALL ] query\n      | query EXCEPT query\n      | query INTERSECT query\n    }\n    [ ORDER BY orderItem [, orderItem ]* ]\n    [ LIMIT { count | ALL } ]\n    [ OFFSET start { ROW | ROWS } ]\n    [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ONLY]\n\nwithItem:\n    name\n    [ '(' column [, column ]* ')' ]\n    AS '(' query ')'\n\norderItem:\n    expression [ ASC | DESC ]\n\nselect:\n    SELECT [ ALL | DISTINCT ]\n    { * | projectItem [, projectItem ]* }\n    FROM tableExpression\n    [ WHERE booleanExpression ]\n    [ GROUP BY { groupItem [, groupItem ]* } ]\n    [ HAVING booleanExpression ]\n    [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ]\n\nselectWithoutFrom:\n    SELECT [ ALL | DISTINCT ]\n    { * | projectItem [, projectItem ]* }\n\nprojectItem:\n    expression [ [ AS ] columnAlias ]\n  | tableAlias . *\n\ntableExpression:\n    tableReference [, tableReference ]*\n  | tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ]\n\njoinCondition:\n    ON booleanExpression\n  | USING '(' column [, column ]* ')'\n\ntableReference:\n    tablePrimary\n    [ matchRecognize ]\n    [ [ AS ] alias [ '(' columnAlias [, columnAlias ]* ')' ] ]\n\ntablePrimary:\n    [ TABLE ] tablePath [ dynamicTableOptions ] [systemTimePeriod] [[AS] correlationName]\n  | LATERAL TABLE '(' functionName '(' expression [, expression ]* ')' ')'\n  | [ LATERAL ] '(' query ')'\n  | UNNEST '(' expression ')'\n\ntablePath:\n    [ [ catalogName . ] databaseName . ] tableName\n\nsystemTimePeriod:\n    FOR SYSTEM_TIME AS OF dateTimeExpression\n\ndynamicTableOptions:\n    /*+ OPTIONS(key=val [, key=val]*) */\n\nkey:\n    stringLiteral\n\nval:\n    stringLiteral\n\nvalues:\n    VALUES expression [, expression ]*\n\ngroupItem:\n    expression\n  | '(' ')'\n  | '(' expression [, expression ]* ')'\n  | CUBE '(' expression [, expression ]* ')'\n  | ROLLUP '(' expression [, expression ]* ')'\n  | GROUPING SETS '(' groupItem [, groupItem ]* ')'\n\nwindowRef:\n    windowName\n  | windowSpec\n\nwindowSpec:\n    [ windowName ]\n    '('\n    [ ORDER BY orderItem [, orderItem ]* ]\n    [ PARTITION BY expression [, expression ]* ]\n    [\n        RANGE numericOrIntervalExpression {PRECEDING}\n      | ROWS numericExpression {PRECEDING}\n    ]\n    ')'\n\nmatchRecognize:\n    MATCH_RECOGNIZE '('\n    [ PARTITION BY expression [, expression ]* ]\n    [ ORDER BY orderItem [, orderItem ]* ]\n    [ MEASURES measureColumn [, measureColumn ]* ]\n    [ ONE ROW PER MATCH ]\n    [ AFTER MATCH\n      ( SKIP TO NEXT ROW\n      | SKIP PAST LAST ROW\n      | SKIP TO FIRST variable\n      | SKIP TO LAST variable\n      | SKIP TO variable )\n    ]\n    PATTERN '(' pattern ')'\n    [ WITHIN intervalLiteral ]\n    DEFINE variable AS condition [, variable AS condition ]*\n    ')'\n\nmeasureColumn:\n    expression AS alias\n\npattern:\n    patternTerm [ '|' patternTerm ]*\n\npatternTerm:\n    patternFactor [ patternFactor ]*\n\npatternFactor:\n    variable [ patternQuantifier ]\n\npatternQuantifier:\n    '*'\n  | '*?'\n  | '+'\n  | '+?'\n  | '?'\n  | '??'\n  | '{' { [ minRepeat ], [ maxRepeat ] } '}' ['?']\n  | '{' repeat '}'\n"
  ],
  "Hints": [
    "table_path /*+ OPTIONS(key=val [, key=val]*) */\n\nkey:\n    stringLiteral\nval:\n    stringLiteral\n",
    "\nCREATE TABLE kafka_table1 (id BIGINT, name STRING, age INT) WITH (...);\nCREATE TABLE kafka_table2 (id BIGINT, name STRING, age INT) WITH (...);\n\n-- override table options in query source\nselect id, name from kafka_table1 /*+ OPTIONS('scan.startup.mode'='earliest-offset') */;\n\n-- override table options in join\nselect * from\n    kafka_table1 /*+ OPTIONS('scan.startup.mode'='earliest-offset') */ t1\n    join\n    kafka_table2 /*+ OPTIONS('scan.startup.mode'='earliest-offset') */ t2\n    on t1.id = t2.id;\n\n-- override table options for INSERT target table\ninsert into kafka_table1 /*+ OPTIONS('sink.partitioner'='round-robin') */ select * from kafka_table2;\n",
    "# Query Hints:\nSELECT /*+ hint [, hint ] */ ...\n\nhint:\n        hintName\n    |   hintName '(' optionKey '=' optionVal [, optionKey '=' optionVal ]* ')'\n    |   hintName '(' hintOption [, hintOption ]* ')'\n\noptionKey:\n        simpleIdentifier\n    |   stringLiteral\n\noptionVal:\n        stringLiteral\n\nhintOption:\n        simpleIdentifier\n    |   numericLiteral\n    |   stringLiteral\n",
    "CREATE TABLE t1 (id BIGINT, name STRING, age INT) WITH (...);\nCREATE TABLE t2 (id BIGINT, name STRING, age INT) WITH (...);\nCREATE TABLE t3 (id BIGINT, name STRING, age INT) WITH (...);\n\n-- Flink will use broadcast join and t1 will be the broadcast table.\nSELECT /*+ BROADCAST(t1) */ * FROM t1 JOIN t2 ON t1.id = t2.id;\n\n-- Flink will use broadcast join for both joins and t1, t3 will be the broadcast table.\nSELECT /*+ BROADCAST(t1, t3) */ * FROM t1 JOIN t2 ON t1.id = t2.id JOIN t3 ON t1.id = t3.id;\n\n-- BROADCAST don't support non-equivalent join conditions.\n-- Join Hint will not work, and only nested loop join can be applied.\nSELECT /*+ BROADCAST(t1) */ * FROM t1 join t2 ON t1.id > t2.id;\n\n-- BROADCAST don't support full outer join.\n-- Join Hint will not work in this case, and the planner will choose the appropriate join strategy based on cost.\nSELECT /*+ BROADCAST(t1) */ * FROM t1 FULL OUTER JOIN t2 ON t1.id = t2.id;\n",
    "CREATE TABLE t1 (id BIGINT, name STRING, age INT) WITH (...);\nCREATE TABLE t2 (id BIGINT, name STRING, age INT) WITH (...);\nCREATE TABLE t3 (id BIGINT, name STRING, age INT) WITH (...);\n\n-- Flink will use hash join and t1 will be the build side.\nSELECT /*+ SHUFFLE_HASH(t1) */ * FROM t1 JOIN t2 ON t1.id = t2.id;\n\n-- Flink will use hash join for both joins and t1, t3 will be the join build side.\nSELECT /*+ SHUFFLE_HASH(t1, t3) */ * FROM t1 JOIN t2 ON t1.id = t2.id JOIN t3 ON t1.id = t3.id;\n\n-- SHUFFLE_HASH don't support non-equivalent join conditions.\n-- For this case, Join Hint will not work, and only nested loop join can be applied.\nSELECT /*+ SHUFFLE_HASH(t1) */ * FROM t1 join t2 ON t1.id > t2.id;\n",
    "CREATE TABLE t1 (id BIGINT, name STRING, age INT) WITH (...);\nCREATE TABLE t2 (id BIGINT, name STRING, age INT) WITH (...);\nCREATE TABLE t3 (id BIGINT, name STRING, age INT) WITH (...);\n\n-- Sort merge join strategy is adopted.\nSELECT /*+ SHUFFLE_MERGE(t1) */ * FROM t1 JOIN t2 ON t1.id = t2.id;\n\n-- Sort merge join strategy is both adopted in these two joins.\nSELECT /*+ SHUFFLE_MERGE(t1, t3) */ * FROM t1 JOIN t2 ON t1.id = t2.id JOIN t3 ON t1.id = t3.id;\n\n-- SHUFFLE_MERGE don't support non-equivalent join conditions.\n-- Join Hint will not work, and only nested loop join can be applied.\nSELECT /*+ SHUFFLE_MERGE(t1) */ * FROM t1 join t2 ON t1.id > t2.id;\n",
    "CREATE TABLE t1 (id BIGINT, name STRING, age INT) WITH (...);\nCREATE TABLE t2 (id BIGINT, name STRING, age INT) WITH (...);\nCREATE TABLE t3 (id BIGINT, name STRING, age INT) WITH (...);\n\n-- Flink will use nested loop join and t1 will be the build side.\nSELECT /*+ NEST_LOOP(t1) */ * FROM t1 JOIN t2 ON t1.id = t2.id;\n\n-- Flink will use nested loop join for both joins and t1, t3 will be the join build side.\nSELECT /*+ NEST_LOOP(t1, t3) */ * FROM t1 JOIN t2 ON t1.id = t2.id JOIN t3 ON t1.id = t3.id;\n",
    "-- suggest the optimizer to use sync lookup\nLOOKUP('table'='Customers', 'async'='false')\n\n-- suggest the optimizer to use async lookup\nLOOKUP('table'='Customers', 'async'='true')\n",
    "-- configure the async parameters: 'output-mode', 'capacity', 'timeout', can set single one or multi params\nLOOKUP('table'='Customers', 'async'='true', 'output-mode'='allow_unordered', 'capacity'='100', 'timeout'='180s')\n",
    "1. LOOKUP('table'='Customers', 'async'='true', 'output-mode'='allow_unordered')\n2. LOOKUP('table'='Customers', 'async'='true', 'timeout'='300s')\n",
    "1. LOOKUP('table'='Customers', 'async'='true', 'output-mode'='allow_unordered', 'capacity'='100', 'timeout'='180s')\n2. LOOKUP('table'='Customers', 'async'='true', 'output-mode'='ordered', 'capacity'='100', 'timeout'='300s')\n",
    "LOOKUP('table'='Customers', 'async'='true', 'retry-predicate'='lookup_miss', 'retry-strategy'='fixed_delay', 'fixed-delay'='10s','max-attempts'='3')\n",
    "LOOKUP('table'='Customers', 'async'='false', 'retry-predicate'='lookup_miss', 'retry-strategy'='fixed_delay', 'fixed-delay'='10s','max-attempts'='3')\n",
    "LOOKUP('table'='Customers', 'retry-predicate'='lookup_miss', 'retry-strategy'='fixed_delay', 'fixed-delay'='10s','max-attempts'='3')\n",
    "SELECT o.order_id, o.total, c.country, c.zip\nFROM Orders AS o\n  JOIN Customers FOR SYSTEM_TIME AS OF o.proc_time AS c\n    ON o.customer_id = c.id\n",
    "SELECT o.order_id, o.total, c.country, c.zip\nFROM Orders AS o\n  JOIN Customers FOR SYSTEM_TIME AS OF o.proc_time AS c\n    ON o.customer_id = c.id and c.country = 'US'\n",
    "CREATE TEMPORARY TABLE Customers (\n  id INT,\n  name STRING,\n  country STRING,\n  zip STRING\n) WITH (\n  'connector' = 'jdbc',\n  'url' = 'jdbc:mysql://mysqlhost:3306/customerdb',\n  'table-name' = 'customers'\n)\n",
    "CREATE TEMPORARY TABLE Customers (\n  id INT,\n  name STRING,\n  country STRING,\n  zip STRING,\n  PRIMARY KEY (id) NOT ENFORCED\n) WITH (\n  'connector' = 'hbase-2.2',\n  ...\n)\n",
    "CREATE TEMPORARY TABLE Customers (\n  id INT,\n  name STRING,\n  country STRING,\n  zip STRING,\n  -- the newly added time-dependent version field\n  update_version STRING\n) WITH (\n  'connector' = 'jdbc',\n  'url' = 'jdbc:mysql://mysqlhost:3306/customerdb',\n  'table-name' = 'customers'\n)\n",
    "ON o.customer_id = c.id AND DATE_FORMAT(o.order_timestamp, 'yyyy-MM-dd HH:mm') = c.update_version\n",
    "CREATE TABLE t1 (id BIGINT, name STRING, age INT) WITH (...);\nCREATE TABLE t2 (id BIGINT, name STRING, age INT) WITH (...);\nCREATE TABLE t3 (id BIGINT, name STRING, age INT) WITH (...);\n\n-- Conflict in One Same Join Hints Strategy Case\n\n-- The first hint will be chosen, t2 will be the broadcast table.\nSELECT /*+ BROADCAST(t2), BROADCAST(t1) */ * FROM t1 JOIN t2 ON t1.id = t2.id;\n\n-- BROADCAST(t2, t1) will be chosen, and t2 will be the broadcast table.\nSELECT /*+ BROADCAST(t2, t1), BROADCAST(t1, t2) */ * FROM t1 JOIN t2 ON t1.id = t2.id;\n\n-- This case equals to BROADCAST(t1, t2) + BROADCAST(t3),\n-- when join between t1 and t2, t1 will be the broadcast table,\n-- when join between the result after t1 join t2 and t3, t3 will be the broadcast table.\nSELECT /*+ BROADCAST(t1, t2, t3) */ * FROM t1 JOIN t2 ON t1.id = t2.id JOIN t3 ON t1.id = t3.id;\n\n\n-- Conflict in Different Join Hints Strategies Case\n\n-- The first Join Hint (BROADCAST(t1)) will be chosen, and t1 will be the broadcast table.\nSELECT /*+ BROADCAST(t1) SHUFFLE_HASH(t1) */ * FROM t1 JOIN t2 ON t1.id = t2.id;\n\n-- Although BROADCAST is first one hint, but it doesn't support full outer join,\n-- so the following SHUFFLE_HASH(t1) will be chosen, and t1 will be the join build side.\nSELECT /*+ BROADCAST(t1) SHUFFLE_HASH(t1) */ * FROM t1 FULL OUTER JOIN t2 ON t1.id = t2.id;\n\n-- Although there are two Join Hints were defined, but all of them are neither support non-equivalent join,\n-- so only nested loop join can be applied.\nSELECT /*+ BROADCAST(t1) SHUFFLE_HASH(t1) */ * FROM t1 FULL OUTER JOIN t2 ON t1.id > t2.id;\n"
  ],
  "WITH clause": [
    "WITH <with_item_definition> [ , ... ]\nSELECT ... FROM ...;\n\n<with_item_defintion>:\n    with_item_name (column_name[, ...n]) AS ( <select_query> )\n",
    "WITH orders_with_total AS (\n    SELECT order_id, price + tax AS total\n    FROM Orders\n)\nSELECT order_id, SUM(total)\nFROM orders_with_total\nGROUP BY order_id;\n"
  ],
  "SELECT & WHERE": [
    "SELECT select_list FROM table_expression [ WHERE boolean_expression ]\n",
    "SELECT * FROM Orders\n",
    "SELECT order_id, price + tax FROM Orders\n",
    "SELECT order_id, price FROM (VALUES (1, 2.0), (2, 3.1))  AS t (order_id, price)\n",
    "SELECT price + tax FROM Orders WHERE id = 10\n",
    "SELECT PRETTY_PRINT(order_id) FROM Orders\n"
  ],
  "SELECT DISTINCT": [
    "SELECT DISTINCT id FROM Orders\n"
  ],
  "Windowing TVF": [
    "TUMBLE(TABLE data, DESCRIPTOR(timecol), size [, offset ])\n",
    "-- tables must have time attribute, e.g. `bidtime` in this table\nFlink SQL> desc Bid;\n+-------------+------------------------+------+-----+--------+---------------------------------+\n|        name |                   type | null | key | extras |                       watermark |\n+-------------+------------------------+------+-----+--------+---------------------------------+\n|     bidtime | TIMESTAMP(3) *ROWTIME* | true |     |        | `bidtime` - INTERVAL '1' SECOND |\n|       price |         DECIMAL(10, 2) | true |     |        |                                 |\n|        item |                 STRING | true |     |        |                                 |\n+-------------+------------------------+------+-----+--------+---------------------------------+\n\nFlink SQL> SELECT * FROM Bid;\n+------------------+-------+------+\n|          bidtime | price | item |\n+------------------+-------+------+\n| 2020-04-15 08:05 |  4.00 | C    |\n| 2020-04-15 08:07 |  2.00 | A    |\n| 2020-04-15 08:09 |  5.00 | D    |\n| 2020-04-15 08:11 |  3.00 | B    |\n| 2020-04-15 08:13 |  1.00 | E    |\n| 2020-04-15 08:17 |  6.00 | F    |\n+------------------+-------+------+\n\nFlink SQL> SELECT * FROM TABLE(\n   TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES));\n-- or with the named params\n-- note: the DATA param must be the first\nFlink SQL> SELECT * FROM TABLE(\n   TUMBLE(\n     DATA => TABLE Bid,\n     TIMECOL => DESCRIPTOR(bidtime),\n     SIZE => INTERVAL '10' MINUTES));\n+------------------+-------+------+------------------+------------------+-------------------------+\n|          bidtime | price | item |     window_start |       window_end |            window_time  |\n+------------------+-------+------+------------------+------------------+-------------------------+\n| 2020-04-15 08:05 |  4.00 | C    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |\n| 2020-04-15 08:07 |  2.00 | A    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |\n| 2020-04-15 08:09 |  5.00 | D    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |\n| 2020-04-15 08:11 |  3.00 | B    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |\n| 2020-04-15 08:13 |  1.00 | E    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |\n| 2020-04-15 08:17 |  6.00 | F    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |\n+------------------+-------+------+------------------+------------------+-------------------------+\n\n-- apply aggregation on the tumbling windowed table\nFlink SQL> SELECT window_start, window_end, SUM(price)\n  FROM TABLE(\n    TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES))\n  GROUP BY window_start, window_end;\n+------------------+------------------+-------+\n|     window_start |       window_end | price |\n+------------------+------------------+-------+\n| 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 |\n+------------------+------------------+-------+\n",
    "HOP(TABLE data, DESCRIPTOR(timecol), slide, size [, offset ])\n",
    "> SELECT * FROM TABLE(\n    HOP(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '5' MINUTES, INTERVAL '10' MINUTES));\n-- or with the named params\n-- note: the DATA param must be the first\n> SELECT * FROM TABLE(\n    HOP(\n      DATA => TABLE Bid,\n      TIMECOL => DESCRIPTOR(bidtime),\n      SLIDE => INTERVAL '5' MINUTES,\n      SIZE => INTERVAL '10' MINUTES));\n+------------------+-------+------+------------------+------------------+-------------------------+\n|          bidtime | price | item |     window_start |       window_end |           window_time   |\n+------------------+-------+------+------------------+------------------+-------------------------+\n| 2020-04-15 08:05 |  4.00 | C    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |\n| 2020-04-15 08:05 |  4.00 | C    | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 |\n| 2020-04-15 08:07 |  2.00 | A    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |\n| 2020-04-15 08:07 |  2.00 | A    | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 |\n| 2020-04-15 08:09 |  5.00 | D    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |\n| 2020-04-15 08:09 |  5.00 | D    | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 |\n| 2020-04-15 08:11 |  3.00 | B    | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 |\n| 2020-04-15 08:11 |  3.00 | B    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |\n| 2020-04-15 08:13 |  1.00 | E    | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 |\n| 2020-04-15 08:13 |  1.00 | E    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |\n| 2020-04-15 08:17 |  6.00 | F    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |\n| 2020-04-15 08:17 |  6.00 | F    | 2020-04-15 08:15 | 2020-04-15 08:25 | 2020-04-15 08:24:59.999 |\n+------------------+-------+------+------------------+------------------+-------------------------+\n\n-- apply aggregation on the hopping windowed table\n> SELECT window_start, window_end, SUM(price)\n  FROM TABLE(\n    HOP(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '5' MINUTES, INTERVAL '10' MINUTES))\n  GROUP BY window_start, window_end;\n+------------------+------------------+-------+\n|     window_start |       window_end | price |\n+------------------+------------------+-------+\n| 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 |\n| 2020-04-15 08:05 | 2020-04-15 08:15 | 15.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 |\n| 2020-04-15 08:15 | 2020-04-15 08:25 |  6.00 |\n+------------------+------------------+-------+\n",
    "CUMULATE(TABLE data, DESCRIPTOR(timecol), step, size)\n",
    "> SELECT * FROM TABLE(\n    CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '2' MINUTES, INTERVAL '10' MINUTES));\n-- or with the named params\n-- note: the DATA param must be the first\n> SELECT * FROM TABLE(\n    CUMULATE(\n      DATA => TABLE Bid,\n      TIMECOL => DESCRIPTOR(bidtime),\n      STEP => INTERVAL '2' MINUTES,\n      SIZE => INTERVAL '10' MINUTES));\n+------------------+-------+------+------------------+------------------+-------------------------+\n|          bidtime | price | item |     window_start |       window_end |            window_time  |\n+------------------+-------+------+------------------+------------------+-------------------------+\n| 2020-04-15 08:05 |  4.00 | C    | 2020-04-15 08:00 | 2020-04-15 08:06 | 2020-04-15 08:05:59.999 |\n| 2020-04-15 08:05 |  4.00 | C    | 2020-04-15 08:00 | 2020-04-15 08:08 | 2020-04-15 08:07:59.999 |\n| 2020-04-15 08:05 |  4.00 | C    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |\n| 2020-04-15 08:07 |  2.00 | A    | 2020-04-15 08:00 | 2020-04-15 08:08 | 2020-04-15 08:07:59.999 |\n| 2020-04-15 08:07 |  2.00 | A    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |\n| 2020-04-15 08:09 |  5.00 | D    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |\n| 2020-04-15 08:11 |  3.00 | B    | 2020-04-15 08:10 | 2020-04-15 08:12 | 2020-04-15 08:11:59.999 |\n| 2020-04-15 08:11 |  3.00 | B    | 2020-04-15 08:10 | 2020-04-15 08:14 | 2020-04-15 08:13:59.999 |\n| 2020-04-15 08:11 |  3.00 | B    | 2020-04-15 08:10 | 2020-04-15 08:16 | 2020-04-15 08:15:59.999 |\n| 2020-04-15 08:11 |  3.00 | B    | 2020-04-15 08:10 | 2020-04-15 08:18 | 2020-04-15 08:17:59.999 |\n| 2020-04-15 08:11 |  3.00 | B    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |\n| 2020-04-15 08:13 |  1.00 | E    | 2020-04-15 08:10 | 2020-04-15 08:14 | 2020-04-15 08:13:59.999 |\n| 2020-04-15 08:13 |  1.00 | E    | 2020-04-15 08:10 | 2020-04-15 08:16 | 2020-04-15 08:15:59.999 |\n| 2020-04-15 08:13 |  1.00 | E    | 2020-04-15 08:10 | 2020-04-15 08:18 | 2020-04-15 08:17:59.999 |\n| 2020-04-15 08:13 |  1.00 | E    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |\n| 2020-04-15 08:17 |  6.00 | F    | 2020-04-15 08:10 | 2020-04-15 08:18 | 2020-04-15 08:17:59.999 |\n| 2020-04-15 08:17 |  6.00 | F    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |\n+------------------+-------+------+------------------+------------------+-------------------------+\n\n-- apply aggregation on the cumulating windowed table\n> SELECT window_start, window_end, SUM(price)\n  FROM TABLE(\n    CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '2' MINUTES, INTERVAL '10' MINUTES))\n  GROUP BY window_start, window_end;\n+------------------+------------------+-------+\n|     window_start |       window_end | price |\n+------------------+------------------+-------+\n| 2020-04-15 08:00 | 2020-04-15 08:06 |  4.00 |\n| 2020-04-15 08:00 | 2020-04-15 08:08 |  6.00 |\n| 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:12 |  3.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:14 |  4.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:16 |  4.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:18 | 10.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 |\n+------------------+------------------+-------+\n",
    "-- NOTE: Currently Flink doesn't support evaluating individual window table-valued function,\n--  window table-valued function should be used with aggregate operation,\n--  this example is just used for explaining the syntax and the data produced by table-valued function.\nFlink SQL> SELECT * FROM TABLE(\n   TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES, INTERVAL '1' MINUTES));\n-- or with the named params\n-- note: the DATA param must be the first\nFlink SQL> SELECT * FROM TABLE(\n   TUMBLE(\n     DATA => TABLE Bid,\n     TIMECOL => DESCRIPTOR(bidtime),\n     SIZE => INTERVAL '10' MINUTES,\n     OFFSET => INTERVAL '1' MINUTES));\n+------------------+-------+------+------------------+------------------+-------------------------+\n|          bidtime | price | item |     window_start |       window_end |            window_time  |\n+------------------+-------+------+------------------+------------------+-------------------------+\n| 2020-04-15 08:05 |  4.00 | C    | 2020-04-15 08:01 | 2020-04-15 08:11 | 2020-04-15 08:10:59.999 |\n| 2020-04-15 08:07 |  2.00 | A    | 2020-04-15 08:01 | 2020-04-15 08:11 | 2020-04-15 08:10:59.999 |\n| 2020-04-15 08:09 |  5.00 | D    | 2020-04-15 08:01 | 2020-04-15 08:11 | 2020-04-15 08:10:59.999 |\n| 2020-04-15 08:11 |  3.00 | B    | 2020-04-15 08:11 | 2020-04-15 08:21 | 2020-04-15 08:20:59.999 |\n| 2020-04-15 08:13 |  1.00 | E    | 2020-04-15 08:11 | 2020-04-15 08:21 | 2020-04-15 08:20:59.999 |\n| 2020-04-15 08:17 |  6.00 | F    | 2020-04-15 08:11 | 2020-04-15 08:21 | 2020-04-15 08:20:59.999 |\n+------------------+-------+------+------------------+------------------+-------------------------+\n\n-- apply aggregation on the tumbling windowed table\nFlink SQL> SELECT window_start, window_end, SUM(price)\n  FROM TABLE(\n    TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES, INTERVAL '1' MINUTES))\n  GROUP BY window_start, window_end;\n+------------------+------------------+-------+\n|     window_start |       window_end | price |\n+------------------+------------------+-------+\n| 2020-04-15 08:01 | 2020-04-15 08:11 | 11.00 |\n| 2020-04-15 08:11 | 2020-04-15 08:21 | 10.00 |\n+------------------+------------------+-------+\n"
  ],
  "Window Aggregation": [
    "SELECT ...\nFROM <windowed_table> -- relation applied windowing TVF\nGROUP BY window_start, window_end, ...\n",
    "-- tables must have time attribute, e.g. `bidtime` in this table\nFlink SQL> desc Bid;\n+-------------+------------------------+------+-----+--------+---------------------------------+\n|        name |                   type | null | key | extras |                       watermark |\n+-------------+------------------------+------+-----+--------+---------------------------------+\n|     bidtime | TIMESTAMP(3) *ROWTIME* | true |     |        | `bidtime` - INTERVAL '1' SECOND |\n|       price |         DECIMAL(10, 2) | true |     |        |                                 |\n|        item |                 STRING | true |     |        |                                 |\n| supplier_id |                 STRING | true |     |        |                                 |\n+-------------+------------------------+------+-----+--------+---------------------------------+\n\nFlink SQL> SELECT * FROM Bid;\n+------------------+-------+------+-------------+\n|          bidtime | price | item | supplier_id |\n+------------------+-------+------+-------------+\n| 2020-04-15 08:05 | 4.00  | C    | supplier1   |\n| 2020-04-15 08:07 | 2.00  | A    | supplier1   |\n| 2020-04-15 08:09 | 5.00  | D    | supplier2   |\n| 2020-04-15 08:11 | 3.00  | B    | supplier2   |\n| 2020-04-15 08:13 | 1.00  | E    | supplier1   |\n| 2020-04-15 08:17 | 6.00  | F    | supplier2   |\n+------------------+-------+------+-------------+\n\n-- tumbling window aggregation\nFlink SQL> SELECT window_start, window_end, SUM(price)\n  FROM TABLE(\n    TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES))\n  GROUP BY window_start, window_end;\n+------------------+------------------+-------+\n|     window_start |       window_end | price |\n+------------------+------------------+-------+\n| 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 |\n+------------------+------------------+-------+\n\n-- hopping window aggregation\nFlink SQL> SELECT window_start, window_end, SUM(price)\n  FROM TABLE(\n    HOP(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '5' MINUTES, INTERVAL '10' MINUTES))\n  GROUP BY window_start, window_end;\n+------------------+------------------+-------+\n|     window_start |       window_end | price |\n+------------------+------------------+-------+\n| 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 |\n| 2020-04-15 08:05 | 2020-04-15 08:15 | 15.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 |\n| 2020-04-15 08:15 | 2020-04-15 08:25 | 6.00  |\n+------------------+------------------+-------+\n\n-- cumulative window aggregation\nFlink SQL> SELECT window_start, window_end, SUM(price)\n  FROM TABLE(\n    CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '2' MINUTES, INTERVAL '10' MINUTES))\n  GROUP BY window_start, window_end;\n+------------------+------------------+-------+\n|     window_start |       window_end | price |\n+------------------+------------------+-------+\n| 2020-04-15 08:00 | 2020-04-15 08:06 | 4.00  |\n| 2020-04-15 08:00 | 2020-04-15 08:08 | 6.00  |\n| 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:12 | 3.00  |\n| 2020-04-15 08:10 | 2020-04-15 08:14 | 4.00  |\n| 2020-04-15 08:10 | 2020-04-15 08:16 | 4.00  |\n| 2020-04-15 08:10 | 2020-04-15 08:18 | 10.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 |\n+------------------+------------------+-------+\n",
    "Flink SQL> SELECT window_start, window_end, supplier_id, SUM(price) as price\n  FROM TABLE(\n    TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES))\n  GROUP BY window_start, window_end, GROUPING SETS ((supplier_id), ());\n+------------------+------------------+-------------+-------+\n|     window_start |       window_end | supplier_id | price |\n+------------------+------------------+-------------+-------+\n| 2020-04-15 08:00 | 2020-04-15 08:10 |      (NULL) | 11.00 |\n| 2020-04-15 08:00 | 2020-04-15 08:10 |   supplier2 |  5.00 |\n| 2020-04-15 08:00 | 2020-04-15 08:10 |   supplier1 |  6.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 |      (NULL) | 10.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 |   supplier2 |  9.00 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 |   supplier1 |  1.00 |\n+------------------+------------------+-------------+-------+\n",
    "SELECT window_start, window_end, supplier_id, SUM(price) as price\nFROM TABLE(\n    TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES))\nGROUP BY window_start, window_end, ROLLUP (supplier_id);\n",
    "SELECT window_start, window_end, item, supplier_id, SUM(price) as price\n  FROM TABLE(\n    TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES))\n  GROUP BY window_start, window_end, CUBE (supplier_id, item);\n\nSELECT window_start, window_end, item, supplier_id, SUM(price) as price\n  FROM TABLE(\n    TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES))\n  GROUP BY window_start, window_end, GROUPING SETS (\n      (supplier_id, item),\n      (supplier_id      ),\n      (             item),\n      (                 )\n)\n",
    "-- tumbling 5 minutes for each supplier_id\nCREATE VIEW window1 AS\n-- Note: The window start and window end fields of inner Window TVF are optional in the select clause. However, if they appear in the clause, they need to be aliased to prevent name conflicting with the window start and window end of the outer Window TVF.\nSELECT window_start as window_5mintumble_start, window_end as window_5mintumble_end, window_time as rowtime, SUM(price) as partial_price\n  FROM TABLE(\n    TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '5' MINUTES))\n  GROUP BY supplier_id, window_start, window_end, window_time;\n\n-- tumbling 10 minutes on the first window\nSELECT window_start, window_end, SUM(partial_price) as total_price\n  FROM TABLE(\n      TUMBLE(TABLE window1, DESCRIPTOR(rowtime), INTERVAL '10' MINUTES))\n  GROUP BY window_start, window_end;\n",
    "CREATE TABLE Orders (\n  user       BIGINT,\n  product    STRING,\n  amount     INT,\n  order_time TIMESTAMP(3),\n  WATERMARK FOR order_time AS order_time - INTERVAL '1' MINUTE\n) WITH (...);\n\nSELECT\n  user,\n  TUMBLE_START(order_time, INTERVAL '1' DAY) AS wStart,\n  SUM(amount) FROM Orders\nGROUP BY\n  TUMBLE(order_time, INTERVAL '1' DAY),\n  user\n"
  ],
  "Group Aggregation": [
    "SELECT COUNT(*) FROM Orders\n",
    "SELECT COUNT(*)\nFROM Orders\nGROUP BY order_id\n",
    "SELECT COUNT(DISTINCT order_id) FROM Orders\n",
    "SELECT supplier_id, rating, COUNT(*) AS total\nFROM (VALUES\n    ('supplier1', 'product1', 4),\n    ('supplier1', 'product2', 3),\n    ('supplier2', 'product3', 3),\n    ('supplier2', 'product4', 4))\nAS Products(supplier_id, product_id, rating)\nGROUP BY GROUPING SETS ((supplier_id, rating), (supplier_id), ())\n",
    "SELECT supplier_id, rating, COUNT(*)\nFROM (VALUES\n    ('supplier1', 'product1', 4),\n    ('supplier1', 'product2', 3),\n    ('supplier2', 'product3', 3),\n    ('supplier2', 'product4', 4))\nAS Products(supplier_id, product_id, rating)\nGROUP BY ROLLUP (supplier_id, rating)\n",
    "SELECT supplier_id, rating, product_id, COUNT(*)\nFROM (VALUES\n    ('supplier1', 'product1', 4),\n    ('supplier1', 'product2', 3),\n    ('supplier2', 'product3', 3),\n    ('supplier2', 'product4', 4))\nAS Products(supplier_id, product_id, rating)\nGROUP BY CUBE (supplier_id, rating, product_id)\n\nSELECT supplier_id, rating, product_id, COUNT(*)\nFROM (VALUES\n    ('supplier1', 'product1', 4),\n    ('supplier1', 'product2', 3),\n    ('supplier2', 'product3', 3),\n    ('supplier2', 'product4', 4))\nAS Products(supplier_id, product_id, rating)\nGROUP BY GROUPING SET (\n    ( supplier_id, product_id, rating ),\n    ( supplier_id, product_id         ),\n    ( supplier_id,             rating ),\n    ( supplier_id                     ),\n    (              product_id, rating ),\n    (              product_id         ),\n    (                          rating ),\n    (                                 )\n)\n",
    "SELECT SUM(amount)\nFROM Orders\nGROUP BY users\nHAVING SUM(amount) > 50\n"
  ],
  "Over Aggregation": [
    "SELECT order_id, order_time, amount,\n  SUM(amount) OVER (\n    PARTITION BY product\n    ORDER BY order_time\n    RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW\n  ) AS one_hour_prod_amount_sum\nFROM Orders\n",
    "SELECT\n  agg_func(agg_col) OVER (\n    [PARTITION BY col1[, col2, ...]]\n    ORDER BY time_col\n    range_definition),\n  ...\nFROM ...\n",
    "RANGE BETWEEN INTERVAL '30' MINUTE PRECEDING AND CURRENT ROW\n",
    "ROWS BETWEEN 10 PRECEDING AND CURRENT ROW\nWINDOW\n",
    "SELECT order_id, order_time, amount,\n  SUM(amount) OVER w AS sum_amount,\n  AVG(amount) OVER w AS avg_amount\nFROM Orders\nWINDOW w AS (\n  PARTITION BY product\n  ORDER BY order_time\n  RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW)\n"
  ],
  "Joins": [
    "SELECT * FROM Orders\nINNER JOIN Product\nON Orders.productId = Product.id\n",
    "SELECT *\nFROM Orders\nINNER JOIN Product\nON Orders.product_id = Product.id\n",
    "SELECT *\nFROM Orders\nLEFT JOIN Product\nON Orders.product_id = Product.id\n\nSELECT *\nFROM Orders\nRIGHT JOIN Product\nON Orders.product_id = Product.id\n\nSELECT *\nFROM Orders\nFULL OUTER JOIN Product\nON Orders.product_id = Product.id\n",
    "SELECT *\nFROM Orders o, Shipments s\nWHERE o.id = s.order_id\nAND o.order_time BETWEEN s.ship_time - INTERVAL '4' HOUR AND s.ship_time\n",
    "SELECT [column_list]\nFROM table1 [AS <alias1>]\n[LEFT] JOIN table2 FOR SYSTEM_TIME AS OF table1.{ proctime | rowtime } [AS <alias2>]\nON table1.column-name1 = table2.column-name1\n",
    "-- Create a table of orders. This is a standard\n-- append-only dynamic table.\nCREATE TABLE orders (\n    order_id    STRING,\n    price       DECIMAL(32,2),\n    currency    STRING,\n    order_time  TIMESTAMP(3),\n    WATERMARK FOR order_time AS order_time - INTERVAL '15' SECOND\n) WITH (/* ... */);\n\n-- Define a versioned table of currency rates. \n-- This could be from a change-data-capture\n-- such as Debezium, a compacted Kafka topic, or any other\n-- way of defining a versioned table. \nCREATE TABLE currency_rates (\n    currency STRING,\n    conversion_rate DECIMAL(32, 2),\n    update_time TIMESTAMP(3) METADATA FROM `values.source.timestamp` VIRTUAL,\n    WATERMARK FOR update_time AS update_time - INTERVAL '15' SECOND,\n    PRIMARY KEY(currency) NOT ENFORCED\n) WITH (\n   'connector' = 'kafka',\n   'value.format' = 'debezium-json',\n   /* ... */\n);\n\nSELECT \n     order_id,\n     price,\n     orders.currency,\n     conversion_rate,\n     order_time\nFROM orders\nLEFT JOIN currency_rates FOR SYSTEM_TIME AS OF orders.order_time\nON orders.currency = currency_rates.currency;\n\norder_id  price  currency  conversion_rate  order_time\n========  =====  ========  ===============  =========\no_001     11.11  EUR       1.14             12:00:00\no_002     12.51  EUR       1.10             12:06:00\n",
    "10:15> SELECT * FROM LatestRates;\n\ncurrency   rate\n======== ======\nUS Dollar   102\nEuro        114\nYen           1\n\n10:30> SELECT * FROM LatestRates;\n\ncurrency   rate\n======== ======\nUS Dollar   102\nEuro        114\nYen           1\n\n10:52> SELECT * FROM LatestRates;\n\ncurrency   rate\n======== ======\nUS Dollar   102\nEuro        116     <==== changed from 114 to 116\nYen           1\n",
    "SELECT * FROM Orders;\n\namount currency\n====== =========\n     2 Euro             <== arrived at time 10:15\n     1 US Dollar        <== arrived at time 10:30\n     2 Euro             <== arrived at time 10:52\n",
    "SELECT\n  o_amount, r_rate\nFROM\n  Orders,\n  LATERAL TABLE (Rates(o_proctime))\nWHERE\n  r_currency = o_currency\n",
    "SELECT\n  o_amount, r_rate\nFROM\n  Orders,\n  LATERAL TABLE (Rates(o_proctime))\nWHERE\n  r_currency = o_currency\n",
    "-- Customers is backed by the JDBC connector and can be used for lookup joins\nCREATE TEMPORARY TABLE Customers (\n  id INT,\n  name STRING,\n  country STRING,\n  zip STRING\n) WITH (\n  'connector' = 'jdbc',\n  'url' = 'jdbc:mysql://mysqlhost:3306/customerdb',\n  'table-name' = 'customers'\n);\n\n-- enrich each order with customer information\nSELECT o.order_id, o.total, c.country, c.zip\nFROM Orders AS o\n  JOIN Customers FOR SYSTEM_TIME AS OF o.proc_time AS c\n    ON o.customer_id = c.id;\n",
    "SELECT order_id, tag\nFROM Orders CROSS JOIN UNNEST(tags) AS t (tag)\n",
    "SELECT order_id, res\nFROM Orders,\nLATERAL TABLE(table_func(order_id)) t(res)\n",
    "SELECT order_id, res\nFROM Orders\nLEFT OUTER JOIN LATERAL TABLE(table_func(order_id)) t(res)\n  ON TRUE\n"
  ],
  "Window JOIN": [
    "SELECT ...\nFROM L [LEFT|RIGHT|FULL OUTER] JOIN R -- L and R are relations applied windowing TVF\nON L.window_start = R.window_start AND L.window_end = R.window_end AND ...\n",
    "Flink SQL> desc LeftTable;\n+----------+------------------------+------+-----+--------+----------------------------------+\n|     name |                   type | null | key | extras |                        watermark |\n+----------+------------------------+------+-----+--------+----------------------------------+\n| row_time | TIMESTAMP(3) *ROWTIME* | true |     |        | `row_time` - INTERVAL '1' SECOND |\n|      num |                    INT | true |     |        |                                  |\n|       id |                 STRING | true |     |        |                                  |\n+----------+------------------------+------+-----+--------+----------------------------------+\n\nFlink SQL> SELECT * FROM LeftTable;\n+------------------+-----+----+\n|         row_time | num | id |\n+------------------+-----+----+\n| 2020-04-15 12:02 |   1 | L1 |\n| 2020-04-15 12:06 |   2 | L2 |\n| 2020-04-15 12:03 |   3 | L3 |\n+------------------+-----+----+\n\nFlink SQL> desc RightTable;\n+----------+------------------------+------+-----+--------+----------------------------------+\n|     name |                   type | null | key | extras |                        watermark |\n+----------+------------------------+------+-----+--------+----------------------------------+\n| row_time | TIMESTAMP(3) *ROWTIME* | true |     |        | `row_time` - INTERVAL '1' SECOND |\n|      num |                    INT | true |     |        |                                  |\n|       id |                 STRING | true |     |        |                                  |\n+----------+------------------------+------+-----+--------+----------------------------------+\n\nFlink SQL> SELECT * FROM RightTable;\n+------------------+-----+----+\n|         row_time | num | id |\n+------------------+-----+----+\n| 2020-04-15 12:01 |   2 | R2 |\n| 2020-04-15 12:04 |   3 | R3 |\n| 2020-04-15 12:05 |   4 | R4 |\n+------------------+-----+----+\n\nFlink SQL> SELECT L.num as L_Num, L.id as L_Id, R.num as R_Num, R.id as R_Id,\n           COALESCE(L.window_start, R.window_start) as window_start,\n           COALESCE(L.window_end, R.window_end) as window_end\n           FROM (\n               SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL '5' MINUTES))\n           ) L\n           FULL JOIN (\n               SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL '5' MINUTES))\n           ) R\n           ON L.num = R.num AND L.window_start = R.window_start AND L.window_end = R.window_end;\n+-------+------+-------+------+------------------+------------------+\n| L_Num | L_Id | R_Num | R_Id |     window_start |       window_end |\n+-------+------+-------+------+------------------+------------------+\n|     1 |   L1 |  null | null | 2020-04-15 12:00 | 2020-04-15 12:05 |\n|  null | null |     2 |   R2 | 2020-04-15 12:00 | 2020-04-15 12:05 |\n|     3 |   L3 |     3 |   R3 | 2020-04-15 12:00 | 2020-04-15 12:05 |\n|     2 |   L2 |  null | null | 2020-04-15 12:05 | 2020-04-15 12:10 |\n|  null | null |     4 |   R4 | 2020-04-15 12:05 | 2020-04-15 12:10 |\n+-------+------+-------+------+------------------+------------------+\n",
    "Flink SQL> SELECT *\n           FROM (\n               SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL '5' MINUTES))\n           ) L WHERE L.num IN (\n             SELECT num FROM (   \n               SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL '5' MINUTES))\n             ) R WHERE L.window_start = R.window_start AND L.window_end = R.window_end);\n+------------------+-----+----+------------------+------------------+-------------------------+\n|         row_time | num | id |     window_start |       window_end |            window_time  |\n+------------------+-----+----+------------------+------------------+-------------------------+\n| 2020-04-15 12:03 |   3 | L3 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 |\n+------------------+-----+----+------------------+------------------+-------------------------+\n\nFlink SQL> SELECT *\n           FROM (\n               SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL '5' MINUTES))\n           ) L WHERE EXISTS (\n             SELECT * FROM (\n               SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL '5' MINUTES))\n             ) R WHERE L.num = R.num AND L.window_start = R.window_start AND L.window_end = R.window_end);\n+------------------+-----+----+------------------+------------------+-------------------------+\n|         row_time | num | id |     window_start |       window_end |            window_time  |\n+------------------+-----+----+------------------+------------------+-------------------------+\n| 2020-04-15 12:03 |   3 | L3 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 |\n+------------------+-----+----+------------------+------------------+-------------------------+\n",
    "Flink SQL> SELECT *\n           FROM (\n               SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL '5' MINUTES))\n           ) L WHERE L.num NOT IN (\n             SELECT num FROM (   \n               SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL '5' MINUTES))\n             ) R WHERE L.window_start = R.window_start AND L.window_end = R.window_end);\n+------------------+-----+----+------------------+------------------+-------------------------+\n|         row_time | num | id |     window_start |       window_end |            window_time  |\n+------------------+-----+----+------------------+------------------+-------------------------+\n| 2020-04-15 12:02 |   1 | L1 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 |\n| 2020-04-15 12:06 |   2 | L2 | 2020-04-15 12:05 | 2020-04-15 12:10 | 2020-04-15 12:09:59.999 |\n+------------------+-----+----+------------------+------------------+-------------------------+\n\nFlink SQL> SELECT *\n           FROM (\n               SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL '5' MINUTES))\n           ) L WHERE NOT EXISTS (\n             SELECT * FROM (\n               SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL '5' MINUTES))\n             ) R WHERE L.num = R.num AND L.window_start = R.window_start AND L.window_end = R.window_end);\n+------------------+-----+----+------------------+------------------+-------------------------+\n|         row_time | num | id |     window_start |       window_end |            window_time  |\n+------------------+-----+----+------------------+------------------+-------------------------+\n| 2020-04-15 12:02 |   1 | L1 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 |\n| 2020-04-15 12:06 |   2 | L2 | 2020-04-15 12:05 | 2020-04-15 12:10 | 2020-04-15 12:09:59.999 |\n+------------------+-----+----+------------------+------------------+-------------------------+\n"
  ],
  "Set Operations": [
    "Flink SQL> create view t1(s) as values ('c'), ('a'), ('b'), ('b'), ('c');\nFlink SQL> create view t2(s) as values ('d'), ('e'), ('a'), ('b'), ('b');\n\nFlink SQL> (SELECT s FROM t1) UNION (SELECT s FROM t2);\n+---+\n|  s|\n+---+\n|  c|\n|  a|\n|  b|\n|  d|\n|  e|\n+---+\n\nFlink SQL> (SELECT s FROM t1) UNION ALL (SELECT s FROM t2);\n+---+\n|  c|\n+---+\n|  c|\n|  a|\n|  b|\n|  b|\n|  c|\n|  d|\n|  e|\n|  a|\n|  b|\n|  b|\n+---+\n",
    "Flink SQL> (SELECT s FROM t1) INTERSECT (SELECT s FROM t2);\n+---+\n|  s|\n+---+\n|  a|\n|  b|\n+---+\n\nFlink SQL> (SELECT s FROM t1) INTERSECT ALL (SELECT s FROM t2);\n+---+\n|  s|\n+---+\n|  a|\n|  b|\n|  b|\n+---+\n",
    "Flink SQL> (SELECT s FROM t1) EXCEPT (SELECT s FROM t2);\n+---+\n| s |\n+---+\n| c |\n+---+\n\nFlink SQL> (SELECT s FROM t1) EXCEPT ALL (SELECT s FROM t2);\n+---+\n| s |\n+---+\n| c |\n| c |\n+---+\n",
    "SELECT user, amount\nFROM Orders\nWHERE product IN (\n    SELECT product FROM NewProducts\n)\n",
    "SELECT user, amount\nFROM Orders\nWHERE product EXISTS (\n    SELECT product FROM NewProducts\n)\n"
  ],
  "ORDER BY clause": [
    "SELECT *\nFROM Orders\nORDER BY order_time, order_id\n"
  ],
  "LIMIT clause": [
    "SELECT *\nFROM Orders\nORDER BY orderTime\nLIMIT 3\n"
  ],
  "Top-N": [
    "SELECT [column_list]\nFROM (\n   SELECT [column_list],\n     ROW_NUMBER() OVER ([PARTITION BY col1[, col2...]]\n       ORDER BY col1 [asc|desc][, col2 [asc|desc]...]) AS rownum\n   FROM table_name)\nWHERE rownum <= N [AND conditions]\n",
    "CREATE TABLE ShopSales (\n  product_id   STRING,\n  category     STRING,\n  product_name STRING,\n  sales        BIGINT\n) WITH (...);\n\nSELECT *\nFROM (\n  SELECT *,\n    ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) AS row_num\n  FROM ShopSales)\nWHERE row_num <= 5\n",
    "CREATE TABLE ShopSales (\n  product_id   STRING,\n  category     STRING,\n  product_name STRING,\n  sales        BIGINT\n) WITH (...);\n\n-- omit row_num field from the output\nSELECT product_id, category, product_name, sales\nFROM (\n  SELECT *,\n    ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) AS row_num\n  FROM ShopSales)\nWHERE row_num <= 5\n"
  ],
  "Window Top-N": [
    "SELECT [column_list]\nFROM (\n   SELECT [column_list],\n     ROW_NUMBER() OVER (PARTITION BY window_start, window_end [, col_key1...]\n       ORDER BY col1 [asc|desc][, col2 [asc|desc]...]) AS rownum\n   FROM table_name) -- relation applied windowing TVF\nWHERE rownum <= N [AND conditions]\n",
    "-- tables must have time attribute, e.g. `bidtime` in this table\nFlink SQL> desc Bid;\n+-------------+------------------------+------+-----+--------+---------------------------------+\n|        name |                   type | null | key | extras |                       watermark |\n+-------------+------------------------+------+-----+--------+---------------------------------+\n|     bidtime | TIMESTAMP(3) *ROWTIME* | true |     |        | `bidtime` - INTERVAL '1' SECOND |\n|       price |         DECIMAL(10, 2) | true |     |        |                                 |\n|        item |                 STRING | true |     |        |                                 |\n| supplier_id |                 STRING | true |     |        |                                 |\n+-------------+------------------------+------+-----+--------+---------------------------------+\n\nFlink SQL> SELECT * FROM Bid;\n+------------------+-------+------+-------------+\n|          bidtime | price | item | supplier_id |\n+------------------+-------+------+-------------+\n| 2020-04-15 08:05 |  4.00 |    A |   supplier1 |\n| 2020-04-15 08:06 |  4.00 |    C |   supplier2 |\n| 2020-04-15 08:07 |  2.00 |    G |   supplier1 |\n| 2020-04-15 08:08 |  2.00 |    B |   supplier3 |\n| 2020-04-15 08:09 |  5.00 |    D |   supplier4 |\n| 2020-04-15 08:11 |  2.00 |    B |   supplier3 |\n| 2020-04-15 08:13 |  1.00 |    E |   supplier1 |\n| 2020-04-15 08:15 |  3.00 |    H |   supplier2 |\n| 2020-04-15 08:17 |  6.00 |    F |   supplier5 |\n+------------------+-------+------+-------------+\n\nFlink SQL> SELECT *\n  FROM (\n    SELECT *, ROW_NUMBER() OVER (PARTITION BY window_start, window_end ORDER BY price DESC) as rownum\n    FROM (\n      SELECT window_start, window_end, supplier_id, SUM(price) as price, COUNT(*) as cnt\n      FROM TABLE(\n        TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES))\n      GROUP BY window_start, window_end, supplier_id\n    )\n  ) WHERE rownum <= 3;\n+------------------+------------------+-------------+-------+-----+--------+\n|     window_start |       window_end | supplier_id | price | cnt | rownum |\n+------------------+------------------+-------------+-------+-----+--------+\n| 2020-04-15 08:00 | 2020-04-15 08:10 |   supplier1 |  6.00 |   2 |      1 |\n| 2020-04-15 08:00 | 2020-04-15 08:10 |   supplier4 |  5.00 |   1 |      2 |\n| 2020-04-15 08:00 | 2020-04-15 08:10 |   supplier2 |  4.00 |   1 |      3 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 |   supplier5 |  6.00 |   1 |      1 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 |   supplier2 |  3.00 |   1 |      2 |\n| 2020-04-15 08:10 | 2020-04-15 08:20 |   supplier3 |  2.00 |   1 |      3 |\n+------------------+------------------+-------------+-------+-----+--------+\n",
    "Flink SQL> SELECT *\n  FROM (\n    SELECT bidtime, price, item, supplier_id, window_start, window_end, ROW_NUMBER() OVER (PARTITION BY window_start, window_end ORDER BY price DESC) as rownum\n    FROM TABLE(\n               TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES))\n  ) WHERE rownum <= 3;\n+------------------+-------+------+-------------+------------------+------------------+--------+\n|          bidtime | price | item | supplier_id |     window_start |       window_end | rownum |\n+------------------+-------+------+-------------+------------------+------------------+--------+\n| 2020-04-15 08:05 |  4.00 |    A |   supplier1 | 2020-04-15 08:00 | 2020-04-15 08:10 |      2 |\n| 2020-04-15 08:06 |  4.00 |    C |   supplier2 | 2020-04-15 08:00 | 2020-04-15 08:10 |      3 |\n| 2020-04-15 08:09 |  5.00 |    D |   supplier4 | 2020-04-15 08:00 | 2020-04-15 08:10 |      1 |\n| 2020-04-15 08:11 |  2.00 |    B |   supplier3 | 2020-04-15 08:10 | 2020-04-15 08:20 |      3 |\n| 2020-04-15 08:15 |  3.00 |    H |   supplier2 | 2020-04-15 08:10 | 2020-04-15 08:20 |      2 |\n| 2020-04-15 08:17 |  6.00 |    F |   supplier5 | 2020-04-15 08:10 | 2020-04-15 08:20 |      1 |\n+------------------+-------+------+-------------+------------------+------------------+--------+\n"
  ],
  "Deduplication": [
    "SELECT [column_list]\nFROM (\n   SELECT [column_list],\n     ROW_NUMBER() OVER ([PARTITION BY col1[, col2...]]\n       ORDER BY time_attr [asc|desc]) AS rownum\n   FROM table_name)\nWHERE rownum = 1\n",
    "CREATE TABLE Orders (\n  order_id  STRING,\n  user        STRING,\n  product     STRING,\n  num         BIGINT,\n  proctime AS PROCTIME()\n) WITH (...);\n\n-- remove duplicate rows on order_id and keep the first occurrence row,\n-- because there shouldn't be two orders with the same order_id.\nSELECT order_id, user, product, num\nFROM (\n  SELECT *,\n    ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY proctime ASC) AS row_num\n  FROM Orders)\nWHERE row_num = 1\n"
  ],
  "Window Deduplication": [
    "SELECT [column_list]\nFROM (\n   SELECT [column_list],\n     ROW_NUMBER() OVER (PARTITION BY window_start, window_end [, col_key1...]\n       ORDER BY time_attr [asc|desc]) AS rownum\n   FROM table_name) -- relation applied windowing TVF\nWHERE (rownum = 1 | rownum <=1 | rownum < 2) [AND conditions]\n",
    "-- tables must have time attribute, e.g. `bidtime` in this table\nFlink SQL> DESC Bid;\n+-------------+------------------------+------+-----+--------+---------------------------------+\n|        name |                   type | null | key | extras |                       watermark |\n+-------------+------------------------+------+-----+--------+---------------------------------+\n|     bidtime | TIMESTAMP(3) *ROWTIME* | true |     |        | `bidtime` - INTERVAL '1' SECOND |\n|       price |         DECIMAL(10, 2) | true |     |        |                                 |\n|        item |                 STRING | true |     |        |                                 |\n+-------------+------------------------+------+-----+--------+---------------------------------+\n\nFlink SQL> SELECT * FROM Bid;\n+------------------+-------+------+\n|          bidtime | price | item |\n+------------------+-------+------+\n| 2020-04-15 08:05 |  4.00 | C    |\n| 2020-04-15 08:07 |  2.00 | A    |\n| 2020-04-15 08:09 |  5.00 | D    |\n| 2020-04-15 08:11 |  3.00 | B    |\n| 2020-04-15 08:13 |  1.00 | E    |\n| 2020-04-15 08:17 |  6.00 | F    |\n+------------------+-------+------+\n\nFlink SQL> SELECT *\n  FROM (\n    SELECT bidtime, price, item, supplier_id, window_start, window_end, \n      ROW_NUMBER() OVER (PARTITION BY window_start, window_end ORDER BY bidtime DESC) AS rownum\n    FROM TABLE(\n               TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES))\n  ) WHERE rownum <= 1;\n+------------------+-------+------+-------------+------------------+------------------+--------+\n|          bidtime | price | item | supplier_id |     window_start |       window_end | rownum |\n+------------------+-------+------+-------------+------------------+------------------+--------+\n| 2020-04-15 08:09 |  5.00 |    D |   supplier4 | 2020-04-15 08:00 | 2020-04-15 08:10 |      1 |\n| 2020-04-15 08:17 |  6.00 |    F |   supplier5 | 2020-04-15 08:10 | 2020-04-15 08:20 |      1 |\n+------------------+-------+------+-------------+------------------+------------------+--------+\n"
  ],
  "Pattern Recognition": [
    "SELECT T.aid, T.bid, T.cid\nFROM MyTable\n    MATCH_RECOGNIZE (\n      PARTITION BY userid\n      ORDER BY proctime\n      MEASURES\n        A.id AS aid,\n        B.id AS bid,\n        C.id AS cid\n      PATTERN (A B C)\n      DEFINE\n        A AS name = 'a',\n        B AS name = 'b',\n        C AS name = 'c'\n    ) AS T\n",
    "SELECT *\nFROM Ticker\n    MATCH_RECOGNIZE (\n        PARTITION BY symbol\n        ORDER BY rowtime\n        MEASURES\n            START_ROW.rowtime AS start_tstamp,\n            LAST(PRICE_DOWN.rowtime) AS bottom_tstamp,\n            LAST(PRICE_UP.rowtime) AS end_tstamp\n        ONE ROW PER MATCH\n        AFTER MATCH SKIP TO LAST PRICE_UP\n        PATTERN (START_ROW PRICE_DOWN+ PRICE_UP)\n        DEFINE\n            PRICE_DOWN AS\n                (LAST(PRICE_DOWN.price, 1) IS NULL AND PRICE_DOWN.price < START_ROW.price) OR\n                    PRICE_DOWN.price < LAST(PRICE_DOWN.price, 1),\n            PRICE_UP AS\n                PRICE_UP.price > LAST(PRICE_DOWN.price, 1)\n    ) MR;\n",
    "SELECT *\nFROM Ticker\n    MATCH_RECOGNIZE (\n        PARTITION BY symbol\n        ORDER BY rowtime\n        MEASURES\n            FIRST(A.rowtime) AS start_tstamp,\n            LAST(A.rowtime) AS end_tstamp,\n            AVG(A.price) AS avgPrice\n        ONE ROW PER MATCH\n        AFTER MATCH SKIP PAST LAST ROW\n        PATTERN (A+ B)\n        DEFINE\n            A AS AVG(A.price) < 15\n    ) MR;\n",
    "PATTERN (A B+ C* D)\n",
    "SELECT *\nFROM Ticker\n    MATCH_RECOGNIZE(\n        PARTITION BY symbol\n        ORDER BY rowtime\n        MEASURES\n            C.price AS lastPrice\n        ONE ROW PER MATCH\n        AFTER MATCH SKIP PAST LAST ROW\n        PATTERN (A B* C)\n        DEFINE\n            A AS A.price > 10,\n            B AS B.price < 15,\n            C AS C.price > 12\n    )\n",
    "PATTERN (A B* C)\nDEFINE\n    A AS condA(),\n    B AS condB(),\n    C AS NOT condB()\n",
    "SELECT *\nFROM Ticker\n    MATCH_RECOGNIZE(\n        PARTITION BY symbol\n        ORDER BY rowtime\n        MEASURES\n            C.rowtime AS dropTime,\n            A.price - C.price AS dropDiff\n        ONE ROW PER MATCH\n        AFTER MATCH SKIP PAST LAST ROW\n        PATTERN (A B* C) WITHIN INTERVAL '1' HOUR\n        DEFINE\n            B AS B.price > A.price - 10,\n            C AS C.price < A.price - 10\n    )\n",
    "SELECT *\nFROM Ticker\n    MATCH_RECOGNIZE(\n        PARTITION BY symbol\n        ORDER BY rowtime\n        MEASURES\n            FIRST(A.price) AS startPrice,\n            LAST(A.price) AS topPrice,\n            B.price AS lastPrice\n        ONE ROW PER MATCH\n        PATTERN (A+ B)\n        DEFINE\n            A AS LAST(A.price, 1) IS NULL OR A.price > LAST(A.price, 1),\n            B AS B.price < LAST(A.price)\n    )\n",
    "PATTERN (A B+)\nDEFINE\n  A AS A.price >= 10,\n  B AS B.price > A.price AND SUM(price) < 100 AND SUM(B.price) < 80\n",
    "PATTERN (A B+)\nDEFINE\n  A AS A.price >= 10,\n  B AS (LAST(B.price, 1) IS NULL OR B.price > LAST(B.price, 1)) AND\n       (LAST(B.price, 2) IS NULL OR B.price > 2 * LAST(B.price, 2))\n",
    "PATTERN (A B? C)\nDEFINE\n  B AS B.price < 20,\n  C AS LAST(price, 1) < C.price\n",
    "SELECT *\nFROM Ticker\n    MATCH_RECOGNIZE(\n        PARTITION BY symbol\n        ORDER BY rowtime\n        MEASURES\n            SUM(A.price) AS sumPrice,\n            FIRST(rowtime) AS startTime,\n            LAST(rowtime) AS endTime\n        ONE ROW PER MATCH\n        [AFTER MATCH STRATEGY]\n        PATTERN (A+ C)\n        DEFINE\n            A AS SUM(A.price) < 30\n    )\n",
    "PATTERN (A B+ C)\nDEFINE\n  A as A.price > 10,\n  C as C.price > 20\n",
    "PATTERN (A B+ C)\nDEFINE\n  A as A.price > 10,\n  B as B.price <= 20,\n  C as C.price > 20\n",
    "PATTERN (A B+? C)\nDEFINE\n  A as A.price > 10,\n  C as C.price > 20\n"
  ],
  "CREATE Statements": [
    "Flink SQL> CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...);\n[INFO] Table has been created.\n\nFlink SQL> CREATE TABLE RubberOrders (product STRING, amount INT) WITH (...);\n[INFO] Table has been created.\n\nFlink SQL> INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE '%Rubber%';\n[INFO] Submitting SQL update statement to the cluster...\n",
    "CREATE TABLE MyTable (\n  `user_id` BIGINT,\n  `name` STRING\n) WITH (\n  ...\n);\n",
    "CREATE TABLE MyTable (\n  `user_id` BIGINT,\n  `name` STRING,\n  `record_time` TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'    -- reads and writes a Kafka record's timestamp\n) WITH (\n  'connector' = 'kafka'\n  ...\n);\n",
    "INSERT INTO MyTable SELECT user_id, name, record_time + INTERVAL '1' SECOND FROM MyTable;\n",
    "CREATE TABLE MyTable (\n  `user_id` BIGINT,\n  `name` STRING,\n  `timestamp` TIMESTAMP_LTZ(3) METADATA    -- use column name as metadata key\n) WITH (\n  'connector' = 'kafka'\n  ...\n);\n",
    "CREATE TABLE MyTable (\n  `user_id` BIGINT,\n  `name` STRING,\n  `timestamp` BIGINT METADATA    -- cast the timestamp as BIGINT\n) WITH (\n  'connector' = 'kafka'\n  ...\n);\n",
    "CREATE TABLE MyTable (\n  `timestamp` BIGINT METADATA,       -- part of the query-to-sink schema\n  `offset` BIGINT METADATA VIRTUAL,  -- not part of the query-to-sink schema\n  `user_id` BIGINT,\n  `name` STRING,\n) WITH (\n  'connector' = 'kafka'\n  ...\n);\n",
    "CREATE TABLE MyTable (\n  `user_id` BIGINT,\n  `price` DOUBLE,\n  `quantity` DOUBLE,\n  `cost` AS price * quanitity,  -- evaluate expression and supply the result to queries\n) WITH (\n  'connector' = 'kafka'\n  ...\n);\n",
    "CREATE TABLE Orders (\n    `user` BIGINT,\n    product STRING,\n    order_time TIMESTAMP(3),\n    WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND\n) WITH ( . . . );\n",
    "CREATE TABLE Orders (\n    `user` BIGINT,\n    product STRING,\n    order_time TIMESTAMP(3)\n) WITH ( \n    'connector' = 'kafka',\n    'scan.startup.mode' = 'earliest-offset'\n);\n\nCREATE TABLE Orders_with_watermark (\n    -- Add watermark definition\n    WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND \n) WITH (\n    -- Overwrite the startup-mode\n    'scan.startup.mode' = 'latest-offset'\n)\nLIKE Orders;\n",
    "CREATE TABLE Orders_with_watermark (\n    `user` BIGINT,\n    product STRING,\n    order_time TIMESTAMP(3),\n    WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND \n) WITH (\n    'connector' = 'kafka',\n    'scan.startup.mode' = 'latest-offset'\n);\n",
    "-- A source table stored in a filesystem\nCREATE TABLE Orders_in_file (\n    `user` BIGINT,\n    product STRING,\n    order_time_string STRING,\n    order_time AS to_timestamp(order_time)\n    \n)\nPARTITIONED BY (`user`) \nWITH ( \n    'connector' = 'filesystem',\n    'path' = '...'\n);\n\n-- A corresponding table we want to store in kafka\nCREATE TABLE Orders_in_kafka (\n    -- Add watermark definition\n    WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND \n) WITH (\n    'connector' = 'kafka',\n    ...\n)\nLIKE Orders_in_file (\n    -- Exclude everything besides the computed columns which we need to generate the watermark for.\n    -- We do not want to have the partitions or filesystem options as those do not apply to kafka. \n    EXCLUDING ALL\n    INCLUDING GENERATED\n);\n",
    "CREATE TABLE my_ctas_table\nWITH (\n    'connector' = 'kafka',\n    ...\n)\nAS SELECT id, name, age FROM source_table WHERE mod(id, 10) = 0;\n",
    "CREATE TABLE my_ctas_table (\n    id BIGINT,\n    name STRING,\n    age INT\n) WITH (\n    'connector' = 'kafka',\n    ...\n);\n \nINSERT INTO my_ctas_table SELECT id, name, age FROM source_table WHERE mod(id, 10) = 0;\n",
    "CREATE CATALOG catalog_name\n  WITH (key1=val1, key2=val2, ...)\n",
    "CREATE DATABASE [IF NOT EXISTS] [catalog_name.]db_name\n  [COMMENT database_comment]\n  WITH (key1=val1, key2=val2, ...)\n",
    "CREATE [TEMPORARY] VIEW [IF NOT EXISTS] [catalog_name.][db_name.]view_name\n  [( columnName [, columnName ]* )] [COMMENT view_comment]\n  AS query_expression\n",
    "CREATE [TEMPORARY|TEMPORARY SYSTEM] FUNCTION \n  [IF NOT EXISTS] [catalog_name.][db_name.]function_name \n  AS identifier [LANGUAGE JAVA|SCALA|PYTHON] \n  [USING JAR '<path_to_filename>.jar' [, JAR '<path_to_filename>.jar']* ]\n"
  ],
  "DROP Statements": [
    "Flink SQL> CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...);\n[INFO] Table has been created.\n\nFlink SQL> SHOW TABLES;\nOrders\n\nFlink SQL> DROP TABLE Orders;\n[INFO] Table has been removed.\n\nFlink SQL> SHOW TABLES;\n[INFO] Result was empty.\n",
    "DROP CATALOG [IF EXISTS] catalog_name\n",
    "DROP [TEMPORARY] TABLE [IF EXISTS] [catalog_name.][db_name.]table_name\n",
    "DROP DATABASE [IF EXISTS] [catalog_name.]db_name [ (RESTRICT | CASCADE) ]\n",
    "DROP [TEMPORARY] VIEW  [IF EXISTS] [catalog_name.][db_name.]view_name\n",
    "DROP [TEMPORARY|TEMPORARY SYSTEM] FUNCTION [IF EXISTS] [catalog_name.][db_name.]function_name;\n"
  ],
  "ALTER Statements": [
    "Flink SQL> CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...);\n[INFO] Execute statement succeed.\n\nFlink SQL> ALTER TABLE Orders ADD `order` INT COMMENT 'order identifier' FIRST;\n[INFO] Execute statement succeed.\n\nFlink SQL> DESCRIBE Orders;\n+---------+--------+------+-----+--------+-----------+------------------+\n|    name |   type | null | key | extras | watermark |          comment |\n+---------+--------+------+-----+--------+-----------+------------------+\n|   order |    INT | TRUE |     |        |           | order identifier |\n|    user | BIGINT | TRUE |     |        |           |                  |\n| product | STRING | TRUE |     |        |           |                  |\n|  amount |    INT | TRUE |     |        |           |                  |\n+---------+--------+------+-----+--------+-----------+------------------+\n4 rows in set\n\nFlink SQL> ALTER TABLE Orders ADD (ts TIMESTAMP(3), category STRING AFTER product, PRIMARY KEY(`order`) NOT ENFORCED, WATERMARK FOR ts AS ts - INTERVAL '1' HOUR);\n[INFO] Execute statement succeed. \n\nFlink SQL> DESCRIBE Orders;\n+----------+------------------------+-------+------------+--------+--------------------------+------------------+\n|     name |                   type |  null |        key | extras |                watermark |          comment |\n+----------+------------------------+-------+------------+--------+--------------------------+------------------+\n|    order |                    INT | FALSE | PRI(order) |        |                          | order identifier |\n|     user |                 BIGINT |  TRUE |            |        |                          |                  |\n|  product |                 STRING |  TRUE |            |        |                          |                  |\n| category |                 STRING |  TRUE |            |        |                          |                  |\n|   amount |                    INT |  TRUE |            |        |                          |                  |\n|       ts | TIMESTAMP(3) *ROWTIME* |  TRUE |            |        | `ts` - INTERVAL '1' HOUR |                  |\n+----------+------------------------+-------+------------+--------+--------------------------+------------------+\n6 rows in set\n\nFlink SQL> ALTER TABLE Orders MODIFY (amount DOUBLE NOT NULL, category STRING COMMENT 'category identifier' AFTER `order`, WATERMARK FOR ts AS ts);\n[INFO] Execute statement succeed. \n\nFlink SQL> DESCRIBE Orders;\n+----------+------------------------+-------+------------+--------+-----------+---------------------+\n|     name |                   type |  null |        key | extras | watermark |             comment |\n+----------+------------------------+-------+------------+--------+-----------+---------------------+\n|    order |                    INT | FALSE | PRI(order) |        |           |    order identifier |\n| category |                 STRING |  TRUE |            |        |           | category identifier |\n|     user |                 BIGINT |  TRUE |            |        |           |                     |\n|  product |                 STRING |  TRUE |            |        |           |                     |\n|   amount |                 DOUBLE | FALSE |            |        |           |                     |\n|       ts | TIMESTAMP(3) *ROWTIME* |  TRUE |            |        |      `ts` |                     |\n+----------+------------------------+-------+------------+--------+-----------+---------------------+\n6 rows in set\n\nFlink SQL> ALTER TABLE Orders DROP WATERMARK;\n[INFO] Execute statement succeed.\n\nFlink SQL> DESCRIBE Orders;\n+----------+--------------+-------+------------+--------+-----------+---------------------+\n|     name |         type |  null |        key | extras | watermark |             comment |\n+----------+--------------+-------+------------+--------+-----------+---------------------+\n|    order |          INT | FALSE | PRI(order) |        |           |    order identifier |\n| category |       STRING |  TRUE |            |        |           | category identifier |\n|     user |       BIGINT |  TRUE |            |        |           |                     |\n|  product |       STRING |  TRUE |            |        |           |                     |\n|   amount |       DOUBLE | FALSE |            |        |           |                     |\n|       ts | TIMESTAMP(3) |  TRUE |            |        |           |                     |\n+----------+--------------+-------+------------+--------+-----------+---------------------+\n6 rows in set\n\nFlink SQL> ALTER TABLE Orders DROP (amount, ts, category);\n[INFO] Execute statement succeed.\n\nFlink SQL> DESCRIBE Orders;\n+---------+--------+-------+------------+--------+-----------+------------------+\n|    name |   type |  null |        key | extras | watermark |          comment |\n+---------+--------+-------+------------+--------+-----------+------------------+\n|   order |    INT | FALSE | PRI(order) |        |           | order identifier |\n|    user | BIGINT |  TRUE |            |        |           |                  |\n| product | STRING |  TRUE |            |        |           |                  |\n+---------+--------+-------+------------+--------+-----------+------------------+\n3 rows in set\n\nFlink SQL> ALTER TABLE Orders RENAME `order` to `order_id`;\n[INFO] Execute statement succeed.\n\nFlink SQL> DESCRIBE Orders;\n+----------+--------+-------+---------------+--------+-----------+------------------+\n|     name |   type |  null |           key | extras | watermark |          comment |\n+----------+--------+-------+---------------+--------+-----------+------------------+\n| order_id |    INT | FALSE | PRI(order_id) |        |           | order identifier |\n|     user | BIGINT |  TRUE |               |        |           |                  |\n|  product | STRING |  TRUE |               |        |           |                  |\n+----------+--------+-------+---------------+--------+-----------+------------------+\n3 rows in set\n\nFlink SQL> SHOW TABLES;\n+------------+\n| table name |\n+------------+\n|     Orders |\n+------------+\n1 row in set\n\nFlink SQL> ALTER TABLE Orders RENAME TO NewOrders;\n[INFO] Execute statement succeed.\n\nFlink SQL> SHOW TABLES;\n+------------+\n| table name |\n+------------+\n|  NewOrders |\n+------------+\n1 row in set\n",
    "-- add a new column \nALTER TABLE MyTable ADD category_id STRING COMMENT 'identifier of the category';\n\n-- add columns, constraint, and watermark\nALTER TABLE MyTable ADD (\n    log_ts STRING COMMENT 'log timestamp string' FIRST,\n    ts AS TO_TIMESTAMP(log_ts) AFTER log_ts,\n    PRIMARY KEY (id) NOT ENFORCED,\n    WATERMARK FOR ts AS ts - INTERVAL '3' SECOND\n);\n",
    "-- modify a column type, comment and position\nALTER TABLE MyTable MODIFY measurement double COMMENT 'unit is bytes per second' AFTER `id`;\n\n-- modify definition of column log_ts and ts, primary key, watermark. They must exist in table schema\nALTER TABLE MyTable MODIFY (\n    log_ts STRING COMMENT 'log timestamp string' AFTER `id`,  -- reorder columns\n    ts AS TO_TIMESTAMP(log_ts) AFTER log_ts,\n    PRIMARY KEY (id) NOT ENFORCED,\n    WATERMARK FOR ts AS ts -- modify watermark strategy\n);\n",
    "-- drop a column\nALTER TABLE MyTable DROP measurement;\n\n-- drop columns\nALTER TABLE MyTable DROP (col1, col2, col3);\n\n-- drop primary key\nALTER TABLE MyTable DROP PRIMARY KEY;\n\n-- drop a watermark\nALTER TABLE MyTable DROP WATERMARK;\n",
    "-- rename column\nALTER TABLE MyTable RENAME request_body TO payload;\n\n-- rename table\nALTER TABLE MyTable RENAME TO MyTable2;\n",
    "-- set 'rows-per-second'\nALTER TABLE DataGenSource SET ('rows-per-second' = '10');\n",
    "-- reset 'rows-per-second' to the default value\nALTER TABLE DataGenSource RESET ('rows-per-second');\n",
    "ALTER VIEW [catalog_name.][db_name.]view_name RENAME TO new_view_name\n",
    "ALTER VIEW [catalog_name.][db_name.]view_name AS new_query_expression\n",
    "ALTER DATABASE [catalog_name.]db_name SET (key1=val1, key2=val2, ...)\n",
    "ALTER [TEMPORARY|TEMPORARY SYSTEM] FUNCTION \n  [IF EXISTS] [catalog_name.][db_name.]function_name \n  AS identifier [LANGUAGE JAVA|SCALA|PYTHON]\n"
  ],
  "INSERT Statement": [
    "Flink SQL> CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...);\n[INFO] Table has been created.\n\nFlink SQL> CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...);\n\nFlink SQL> SHOW TABLES;\nOrders\nRubberOrders\n\nFlink SQL> INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE '%Rubber%';\n[INFO] Submitting SQL update statement to the cluster...\n[INFO] Table update statement has been successfully submitted to the cluster:\n",
    "\n[EXECUTE] INSERT { INTO | OVERWRITE } [catalog_name.][db_name.]table_name [PARTITION part_spec] [column_list] select_statement\n\npart_spec:\n  (part_col_name1=val1 [, part_col_name2=val2, ...])\n\ncolumn_list:\n  (col_name1 [, column_name2, ...])\n",
    "-- Creates a partitioned table\nCREATE TABLE country_page_view (user STRING, cnt INT, date STRING, country STRING)\nPARTITIONED BY (date, country)\nWITH (...)\n\n-- Appends rows into the static partition (date='2019-8-30', country='China')\nINSERT INTO country_page_view PARTITION (date='2019-8-30', country='China')\n  SELECT user, cnt FROM page_view_source;\n  \n-- Key word EXECUTE can be added at the beginning of Insert to indicate explicitly that we are going to execute the statement,\n-- it is equivalent to Statement without the key word. \nEXECUTE INSERT INTO country_page_view PARTITION (date='2019-8-30', country='China')\n  SELECT user, cnt FROM page_view_source;\n\n-- Appends rows into partition (date, country), where date is static partition with value '2019-8-30',\n-- country is dynamic partition whose value is dynamic determined by each row.\nINSERT INTO country_page_view PARTITION (date='2019-8-30')\n  SELECT user, cnt, country FROM page_view_source;\n\n-- Overwrites rows into static partition (date='2019-8-30', country='China')\nINSERT OVERWRITE country_page_view PARTITION (date='2019-8-30', country='China')\n  SELECT user, cnt FROM page_view_source;\n\n-- Overwrites rows into partition (date, country), where date is static partition with value '2019-8-30',\n-- country is dynamic partition whose value is dynamic determined by each row.\nINSERT OVERWRITE country_page_view PARTITION (date='2019-8-30')\n  SELECT user, cnt, country FROM page_view_source;\n\n-- Appends rows into the static partition (date='2019-8-30', country='China')\n-- the column cnt is set to NULL\nINSERT INTO country_page_view PARTITION (date='2019-8-30', country='China') (user)\n  SELECT user FROM page_view_source;\n",
    "[EXECUTE] INSERT { INTO | OVERWRITE } [catalog_name.][db_name.]table_name VALUES values_row [, values_row ...]\n\nvalues_row:\n    (val1 [, val2, ...])\n",
    "\nCREATE TABLE students (name STRING, age INT, gpa DECIMAL(3, 2)) WITH (...);\n\nINSERT INTO students\n  VALUES ('fred flintstone', 35, 1.28), ('barney rubble', 32, 2.32);\n",
    "EXECUTE STATEMENT SET\nBEGIN\ninsert_statement;\n...\ninsert_statement;\nEND;\n\ninsert_statement:\n   <insert_from_select>|<insert_from_values>\n",
    "\nCREATE TABLE students (name STRING, age INT, gpa DECIMAL(3, 2)) WITH (...);\n\nEXECUTE STATEMENT SET\nBEGIN\nINSERT INTO students\n  VALUES ('fred flintstone', 35, 1.28), ('barney rubble', 32, 2.32);\nINSERT INTO students\n  VALUES ('fred flintstone', 35, 1.28), ('barney rubble', 32, 2.32);\nEND;\n"
  ],
  "ANALYZE Statements": [
    "Flink SQL> CREATE TABLE Store (\n> `id` BIGINT NOT NULl,\n> `location` VARCHAR(32),\n> `owner` VARCHAR(32)\n> ) with (\n> ...\n> );\n[INFO] Table has been created.\n\nFlink SQL> CREATE TABLE Orders (\n> `id` BIGINT NOT NULl,\n> `product` VARCHAR(32),\n> `amount` INT,\n> `sold_year` BIGINT,\n> `sold_month` BIGINT,\n> `sold_day` BIGINT   \n> ) PARTITIONED BY (`sold_year`, `sold_month`, `sold_day`)\n> ) with (\n> ...\n> );\n[INFO] Table has been created.\n\nFlink SQL> ANALYZE TABLE Store COMPUTE STATISTICS;\n[INFO] Execute statement succeed.\n    \nFlink SQL> ANALYZE TABLE Store COMPUTE STATISTICS FOR ALL COLUMNS;\n[INFO] Execute statement succeed.\n\nFlink SQL> ANALYZE TABLE Store COMPUTE STATISTICS FOR COLUMNS location;\n[INFO] Execute statement succeed.\n    \nFlink SQL> ANALYZE TABLE Orders PARTITION(sold_year='2022', sold_month='1', sold_day='10') COMPUTE STATISTICS;\n[INFO] Execute statement succeed.\n\nFlink SQL> ANALYZE TABLE Orders PARTITION(sold_year='2022', sold_month='1', sold_day) COMPUTE STATISTICS;\n[INFO] Execute statement succeed.\n\nFlink SQL> ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS;\n[INFO] Execute statement succeed.\n\nFlink SQL> ANALYZE TABLE Orders PARTITION(sold_year='2022', sold_month='1', sold_day='10') COMPUTE STATISTICS FOR ALL COLUMNS;\n[INFO] Execute statement succeed.    \n\nFlink SQL> ANALYZE TABLE Orders PARTITION(sold_year='2022', sold_month='1', sold_day) COMPUTE STATISTICS FOR ALL COLUMNS;\n[INFO] Execute statement succeed.\n    \nFlink SQL> ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS;\n[INFO] Execute statement succeed.\n    \nFlink SQL> ANALYZE TABLE Orders PARTITION(sold_year='2022', sold_month='1', sold_day='10') COMPUTE STATISTICS FOR COLUMNS amount;\n[INFO] Execute statement succeed.\n    \nFlink SQL> ANALYZE TABLE Orders PARTITION (sold_year='2022', sold_month='1', sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product;\n[INFO] Execute statement succeed.\n    \nFlink SQL> ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product;\n[INFO] Execute statement succeed.\n",
    "ANALYZE TABLE [catalog_name.][db_name.]table_name PARTITION(partcol1[=val1] [, partcol2[=val2], ...]) COMPUTE STATISTICS [FOR COLUMNS col1 [, col2, ...] | FOR ALL COLUMNS]\n"
  ],
  "DESCRIBE Statements": [
    "Flink SQL> CREATE TABLE Orders (\n>  `user` BIGINT NOT NULl comment 'this is primary key',\n>  product VARCHAR(32),\n>  amount INT,\n>  ts TIMESTAMP(3) comment 'notice: watermark',\n>  ptime AS PROCTIME() comment 'this is a computed column',\n>  PRIMARY KEY(`user`) NOT ENFORCED,\n>  WATERMARK FOR ts AS ts - INTERVAL '1' SECONDS\n> ) with (\n>  ...\n> );\n[INFO] Table has been created.\n\nFlink SQL> DESCRIBE Orders;\n\nFlink SQL> DESC Orders;\n",
    "{ DESCRIBE | DESC } [catalog_name.][db_name.]table_name\n"
  ],
  "EXPLAIN Statements": [
    "Flink SQL> CREATE TABLE MyTable1 (`count` bigint, word VARCHAR(256)) WITH ('connector' = 'datagen');\n[INFO] Table has been created.\n\nFlink SQL> CREATE TABLE MyTable2 (`count` bigint, word VARCHAR(256)) WITH ('connector' = 'datagen');\n[INFO] Table has been created.\n\nFlink SQL> EXPLAIN PLAN FOR SELECT `count`, COUNT(word) FROM\n> ( SELECT `count`, word FROM MyTable1 WHERE word LIKE 'F%' \n>   UNION ALL \n>   SELECT `count`, word FROM MyTable2 ) tmp GROUP BY `count`;\n                                  \nFlink SQL> EXPLAIN ESTIMATED_COST, CHANGELOG_MODE, PLAN_ADVICE, JSON_EXECUTION_PLAN SELECT `count`, COUNT(word) FROM\n> ( SELECT `count`, word FROM MyTable1 WHERE word LIKE 'F%' \n>   UNION ALL \n>   SELECT `count`, word FROM MyTable2 ) tmp GROUP BY `count`;\n",
    "SET 'table.exec.mini-batch.enabled' = 'true';\nSET 'table.exec.mini-batch.allow-latency' = '5s';\nSET 'table.exec.mini-batch.size' = '200';\nSET 'table.optimizer.agg-phase-strategy' = 'OHE_PHASE';\n\nCREATE TABLE MyTable (\n  a BIGINT,\n  b INT NOT NULL,\n  c VARCHAR,\n  d BIGINT\n) WITH (\n  'connector' = 'values',\n  'bounded' = 'false');\n\nEXPLAIN PLAN_ADVICE\nSELECT\n  AVG(a) AS avg_a,\n  COUNT(*) AS cnt,\n  COUNT(b) AS cnt_b,\n  MIN(b) AS min_b,\n  MAX(c) FILTER (WHERE a > 1) AS max_c\nFROM MyTable;\n",
    "CREATE TABLE MyTable (\n  a INT,\n  b BIGINT,\n  c STRING,\n  d INT,\n  `day` AS DATE_FORMAT(CURRENT_TIMESTAMP, 'yyMMdd'),\n  PRIMARY KEY (a, c) NOT ENFORCED\n) WITH (\n  'connector' = 'values',\n  'changelog-mode' = 'I,UA,UB,D'  \n);\n\nCREATE TABLE MySink (\n a INT,\n b BIGINT,\n c STRING,\n PRIMARY KEY (a) NOT ENFORCED\n) WITH (\n 'connector' = 'values',\n 'sink-insert-only' = 'false'\n);\n\nEXPLAIN PLAN_ADVICE\nINSERT INTO MySink\nSELECT a, b, `day`\nFROM MyTable\nWHERE b > 100;\n",
    "CREATE TABLE MyTable (\n  a INT,\n  b BIGINT,\n  c STRING,\n  d INT\n) WITH (\n  'connector' = 'values',\n  'changelog-mode' = 'I'  \n);\n\nEXPLAIN PLAN_ADVICE\nSELECT * FROM MyTable WHERE b > 100;\n",
    "EXPLAIN [([ExplainDetail[, ExplainDetail]*]) | PLAN FOR] <query_statement_or_insert_statement_or_statement_set>\n\nstatement_set:\nEXECUTE STATEMENT SET\nBEGIN\ninsert_statement;\n...\ninsert_statement;\nEND;\n"
  ],
  "USE Statements": [
    "Flink SQL> CREATE CATALOG cat1 WITH (...);\n[INFO] Catalog has been created.\n\nFlink SQL> SHOW CATALOGS;\ndefault_catalog\ncat1\n\nFlink SQL> USE CATALOG cat1;\n\nFlink SQL> SHOW DATABASES;\n\nFlink SQL> CREATE DATABASE db1 WITH (...);\n[INFO] Database has been created.\n\nFlink SQL> SHOW DATABASES;\ndb1\n\nFlink SQL> USE db1;\n\nFlink SQL> USE MODULES hive;\n[INFO] Use modules succeeded!\nFlink SQL> SHOW FULL MODULES;\n+-------------+-------+\n| module name |  used |\n+-------------+-------+\n|        hive |  true |\n|        core | false |\n+-------------+-------+\n2 rows in set\n",
    "USE CATALOG catalog_name\n",
    "USE MODULES module_name1[, module_name2, ...]\n",
    "USE [catalog_name.]database_name\n"
  ],
  "SHOW Statements": [
    "\nFlink SQL> SHOW CATALOGS;\ndefault_catalog\n\nFlink SQL> SHOW DATABASES;\ndefault_database\n\nFlink SQL> CREATE TABLE my_table (...) WITH (...);\n[INFO] Table has been created.\n\nFlink SQL> SHOW TABLES;\nmy_table\n\nFlink SQL> SHOW CREATE TABLE my_table;\nCREATE TABLE `default_catalog`.`default_db`.`my_table` (\n  ...\n) WITH (\n  ...\n)\n\n\nFlink SQL> SHOW COLUMNS from MyUserTable LIKE '%f%';\n+--------+-------+------+-----+--------+-----------+\n|   name |  type | null | key | extras | watermark |\n+--------+-------+------+-----+--------+-----------+\n| field2 | BYTES | true |     |        |           |\n+--------+-------+------+-----+--------+-----------+\n1 row in set\n\n\nFlink SQL> CREATE VIEW my_view AS SELECT * from my_table;\n[INFO] View has been created.\n\nFlink SQL> SHOW VIEWS;\nmy_view\n\nFlink SQL> SHOW CREATE VIEW my_view;\nCREATE VIEW `default_catalog`.`default_db`.`my_view`(`field1`, `field2`, ...) as\nSELECT *\nFROM `default_catalog`.`default_database`.`my_table`\n\nFlink SQL> SHOW FUNCTIONS;\nmod\nsha256\n...\n\nFlink SQL> CREATE FUNCTION f1 AS ...;\n[INFO] Function has been created.\n\nFlink SQL> SHOW USER FUNCTIONS;\nf1\n...\n\nFlink SQL> SHOW MODULES;\n+-------------+\n| module name |\n+-------------+\n|        core |\n+-------------+\n1 row in set\n\n\nFlink SQL> SHOW FULL MODULES;\n+-------------+------+\n| module name | used |\n+-------------+------+\n|        core | true |\n+-------------+------+\n1 row in set\n\n\nFlink SQL> SHOW JARS;\n/path/to/addedJar.jar\n",
    "SHOW CATALOGS\n",
    "SHOW CURRENT CATALOG\n",
    "SHOW DATABASES\n",
    "SHOW CURRENT DATABASE\n",
    "SHOW TABLES [ ( FROM | IN ) [catalog_name.]database_name ] [ [NOT] LIKE <sql_like_pattern> ]\n",
    "show tables from db1;\n-- show tables from catalog1.db1;\n-- show tables in db1;\n-- show tables in catalog1.db1;\n+------------+\n| table name |\n+------------+\n|        dim |\n|     person |\n+------------+\n2 rows in set\n",
    "show tables from db1 like '%n';\n-- show tables from catalog1.db1 like '%n';\n-- show tables in db1 like '%n';\n-- show tables in catalog1.db1 like '%n';\n+------------+\n| table name |\n+------------+\n|     person |\n+------------+\n1 row in set\n",
    "show tables from db1 not like '%n';\n-- show tables from catalog1.db1 not like '%n';\n-- show tables in db1 not like '%n';\n-- show tables in catalog1.db1 not like '%n';\n+------------+\n| table name |\n+------------+\n|        dim |\n+------------+\n1 row in set\n",
    "show tables;\n+------------+\n| table name |\n+------------+\n|      items |\n|     orders |\n+------------+\n2 rows in set\n",
    "SHOW CREATE TABLE\n",
    "SHOW COLUMNS ( FROM | IN ) [[catalog_name.]database.]<table_name> [ [NOT] LIKE <sql_like_pattern>]\n",
    "+---------+-----------------------------+-------+-----------+---------------+----------------------------+\n|    name |                        type |  null |       key |        extras |                  watermark |\n+---------+-----------------------------+-------+-----------+---------------+----------------------------+\n|    user |                      BIGINT | false | PRI(user) |               |                            |\n| product |                 VARCHAR(32) |  true |           |               |                            |\n|  amount |                         INT |  true |           |               |                            |\n|      ts |      TIMESTAMP(3) *ROWTIME* |  true |           |               | `ts` - INTERVAL '1' SECOND |\n|   ptime | TIMESTAMP_LTZ(3) *PROCTIME* | false |           | AS PROCTIME() |                            |\n+---------+-----------------------------+-------+-----------+---------------+----------------------------+\n",
    "show columns from orders;\n-- show columns from database1.orders;\n-- show columns from catalog1.database1.orders;\n-- show columns in orders;\n-- show columns in database1.orders;\n-- show columns in catalog1.database1.orders;\n+---------+-----------------------------+-------+-----------+---------------+----------------------------+\n|    name |                        type |  null |       key |        extras |                  watermark |\n+---------+-----------------------------+-------+-----------+---------------+----------------------------+\n|    user |                      BIGINT | false | PRI(user) |               |                            |\n| product |                 VARCHAR(32) |  true |           |               |                            |\n|  amount |                         INT |  true |           |               |                            |\n|      ts |      TIMESTAMP(3) *ROWTIME* |  true |           |               | `ts` - INTERVAL '1' SECOND |\n|   ptime | TIMESTAMP_LTZ(3) *PROCTIME* | false |           | AS PROCTIME() |                            |\n+---------+-----------------------------+-------+-----------+---------------+----------------------------+\n5 rows in set\n",
    "show columns from orders like '%r';\n-- show columns from database1.orders like '%r';\n-- show columns from catalog1.database1.orders like '%r';\n-- show columns in orders like '%r';\n-- show columns in database1.orders like '%r';\n-- show columns in catalog1.database1.orders like '%r';\n+------+--------+-------+-----------+--------+-----------+\n| name |   type |  null |       key | extras | watermark |\n+------+--------+-------+-----------+--------+-----------+\n| user | BIGINT | false | PRI(user) |        |           |\n+------+--------+-------+-----------+--------+-----------+\n1 row in set\n",
    "show columns from orders not like '%_r';\n-- show columns from database1.orders not like '%_r';\n-- show columns from catalog1.database1.orders not like '%_r';\n-- show columns in orders not like '%_r';\n-- show columns in database1.orders not like '%_r';\n-- show columns in catalog1.database1.orders not like '%_r';\n+---------+-----------------------------+-------+-----+---------------+----------------------------+\n|    name |                        type |  null | key |        extras |                  watermark |\n+---------+-----------------------------+-------+-----+---------------+----------------------------+\n| product |                 VARCHAR(32) |  true |     |               |                            |\n|  amount |                         INT |  true |     |               |                            |\n|      ts |      TIMESTAMP(3) *ROWTIME* |  true |     |               | `ts` - INTERVAL '1' SECOND |\n|   ptime | TIMESTAMP_LTZ(3) *PROCTIME* | false |     | AS PROCTIME() |                            |\n+---------+-----------------------------+-------+-----+---------------+----------------------------+\n4 rows in set\n",
    "SHOW VIEWS\n",
    "SHOW CREATE VIEW [catalog_name.][db_name.]view_name\n",
    "SHOW [USER] FUNCTIONS\n",
    "SHOW [FULL] MODULES\n",
    "SHOW JARS\n"
  ],
  "LOAD Statements": [
    "Flink SQL> LOAD MODULE hive WITH ('hive-version' = '3.1.3');\n[INFO] Load module succeeded!\n\nFlink SQL> SHOW MODULES;\n+-------------+\n| module name |\n+-------------+\n|        core |\n|        hive |\n+-------------+\n",
    "LOAD MODULE module_name [WITH ('key1' = 'val1', 'key2' = 'val2', ...)]\n"
  ],
  "UNLOAD Statements": [
    "Flink SQL> UNLOAD MODULE core;\n[INFO] Unload module succeeded!\n\nFlink SQL> SHOW MODULES;\nEmpty set\n",
    "UNLOAD MODULE module_name\n"
  ],
  "SET Statements": [
    "Flink SQL> SET 'table.local-time-zone' = 'Europe/Berlin';\n[INFO] Session property has been set.\n\nFlink SQL> SET;\n'table.local-time-zone' = 'Europe/Berlin'\n",
    "SET ('key' = 'value')?\n"
  ],
  "RESET Statements": [
    "Flink SQL> RESET 'table.planner';\n[INFO] Session property has been reset.\n\nFlink SQL> RESET;\n[INFO] All session properties have been set to their default values.\n",
    "RESET ('key')?\n"
  ],
  "JAR Statements": [
    "Flink SQL> ADD JAR '/path/hello.jar';\n[INFO] Execute statement succeed.\n\nFlink SQL> ADD JAR 'hdfs:///udf/common-udf.jar';\n[INFO] Execute statement succeed.\n\nFlink SQL> SHOW JARS;\n+----------------------------+\n|                       jars |\n+----------------------------+\n|            /path/hello.jar |\n| hdfs:///udf/common-udf.jar |\n+----------------------------+\n\nFlink SQL> REMOVE JAR '/path/hello.jar';\n[INFO] The specified jar is removed from session classloader.\n",
    "ADD JAR '<path_to_filename>.jar'\n",
    "SHOW JARS\n",
    "REMOVE JAR '<path_to_filename>.jar'\n"
  ]
}